{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Calibration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Import packages and set working directory</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "#from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline,FeatureUnion\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import seaborn as sb\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Get data and Explore</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'DataPrep/MainData_Scaled.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-7250d3f8707d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata_clean\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'DataPrep/MainData_Scaled.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"|\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdata_clean1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'DataPrep/MainData_NotScaled.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"|\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdata_clean2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'DataPrep/MainData_Scaled_OutliersRemoved.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"|\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\geoff\\anaconda3\\envs\\moderndata1\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    686\u001b[0m     )\n\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 688\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    689\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\geoff\\anaconda3\\envs\\moderndata1\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 454\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    455\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\geoff\\anaconda3\\envs\\moderndata1\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    946\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 948\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    949\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    950\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\geoff\\anaconda3\\envs\\moderndata1\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1179\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1180\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1181\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1182\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\geoff\\anaconda3\\envs\\moderndata1\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2008\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2009\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2010\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2011\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'DataPrep/MainData_Scaled.csv'"
     ]
    }
   ],
   "source": [
    "data_clean = pd.read_csv('DataPrep/MainData_Scaled.csv', sep=\"|\", header=0)\n",
    "data_clean1 = pd.read_csv('DataPrep/MainData_NotScaled.csv', sep=\"|\", header=0)\n",
    "data_clean2 = pd.read_csv('DataPrep/MainData_Scaled_OutliersRemoved.csv', sep=\"|\", header=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main = data_clean.loc[:,\"CDD\":\"CITY_REL_WATER\"]\n",
    "df_main1 = data_clean1.loc[:,\"CDD\":\"CITY_REL_WATER\"]\n",
    "df_main2 = data_clean2.loc[:,\"CDD\":\"CITY_REL_WATER\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for the null values\n",
    "df_main.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main1.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_2 = df_main.drop(['URB_AREA_HINTER', 'GDP_PC_REAL_PPP','POP_TOT_GI'], axis=1)\n",
    "#df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main = df_main.where(pd.notna(df_main), df_main.mean(), axis=\"columns\")\n",
    "df_main1 = df_main1.where(pd.notna(df_main1), df_main1.mean(), axis=\"columns\")\n",
    "df_main2 = df_main2.where(pd.notna(df_main2), df_main2.mean(), axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main1.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_main1.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(data_clean1, x=\"CDD\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(data_clean2, x=\"CDD\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pre_process = ColumnTransformer(remainder='passthrough',\n",
    "                                transformers=[('drop_columns', 'drop', ['GDP_PC_REAL_PPP',\n",
    "                                                                        'URB_AREA_HINTER',\n",
    "                                                                        'POP_TOT_GI'\n",
    "                                                                       ]),\n",
    "                                              ('impute_FRAGMENTATION', SimpleImputer(strategy='mean'), ['FRAGMENTATION']),\n",
    "                                              ('impute_T_Y0_14_SH_NAT', SimpleImputer(strategy='mean'), ['T_Y0_14_SH_NAT']),\n",
    "                                              ('impute_T_Y15_64_SH_NAT', SimpleImputer(strategy='mean'), ['T_Y15_64_SH_NAT']),\n",
    "                                              ('impute_T_Y65_MAX_SH_NAT', SimpleImputer(strategy='mean'), ['T_Y65_MAX_SH_NAT']),\n",
    "                                              ('impute_PWM_EX_CORE', SimpleImputer(strategy='mean'), ['PWM_EX_CORE'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable importance/selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new arrays for variable importance scaled data with outliers\n",
    "y_1 = df_main.loc[:,\"CDD\"]\n",
    "X_1 = df_main.loc[:,\"URB_AREA\":\"CITY_REL_WATER\"]\n",
    "X_1train, X_1test, y_1train, y_1test = train_test_split(X_1, y_1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lasso = LassoCV().fit(pre_process.fit_transform(X_1train), y_train)\n",
    "#lasso_pipeline = Pipeline(steps=[('pre_processing',pre_process),\n",
    "#                                ('lasso', LassoCV(cv=5, random_state=0))\n",
    "#                                 ])\n",
    "#lasso_pipeline.fit(X_1train,y_2train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(X_1train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lasso = LassoCV(cv=10, random_state=0,max_iter=10000).fit(X_1train, y_1train)\n",
    "importance = np.abs(lasso.coef_)\n",
    "feature_names = np.array(X_1train.columns)\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.bar(height=importance, x=feature_names)\n",
    "plt.title(\"Feature importances via coefficients\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new arrays for variable importance scaled data with outliers removed\n",
    "y_2 = df_main2.loc[:,\"CDD\"]\n",
    "X_2 = df_main2.loc[:,\"URB_AREA\":\"CITY_REL_WATER\"]\n",
    "X_2train, X_2test, y_2train, y_2test = train_test_split(X_2, y_2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_2 = LassoCV(cv=10, random_state=0,max_iter=10000).fit(X_2train, y_2train)\n",
    "importance_2 = np.abs(lasso_2.coef_)\n",
    "feature_names = np.array(X_2train.columns)\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.bar(height=importance_2, x=feature_names)\n",
    "plt.title(\"Feature importances via coefficients\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array for calibration with outliers\n",
    "y = df_main.loc[:,\"CDD\"]\n",
    "X = df_main.loc[:,\"URB_AREA\":\"CITY_REL_WATER\"]\n",
    "rng = np.random.RandomState(0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=rng)\n",
    "\n",
    "# Array for calibration with outliers removed\n",
    "y_1 = df_main2.loc[:,\"CDD\"]\n",
    "X_1 = df_main2.loc[:,\"URB_AREA\":\"CITY_REL_WATER\"]\n",
    "rng = np.random.RandomState(0)\n",
    "X_1train, X_1test, y_1train, y_1test = train_test_split(X_1, y_1, random_state=rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_process = ColumnTransformer(remainder='passthrough',\n",
    "                                transformers=[('drop_columns', 'drop', ['T_Y0_14_SH_NAT',\n",
    "                                                                        'URB_AREA',\n",
    "                                                                        'T_Y65_MAX_SH_NAT',\n",
    "                                                                        'POP_DEN'\n",
    "                                                                       ])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_process_2 = ColumnTransformer(remainder='passthrough',\n",
    "                                transformers=[('drop_columns', 'drop', ['T_Y0_14_SH_NAT',\n",
    "                                                                        'URB_AREA',\n",
    "                                                                        'T_Y15_64_SH_NAT',\n",
    "                                                                        'CITY_REL_ROADS',\n",
    "                                                                        'TREECOVER_SHARE_CORE'\n",
    "                                                                       ])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " cv_1 = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scale_predictor_variables:\n",
    "    def __init__(self,X_train,y_train,pre_process,cv):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.pre_process = pre_process\n",
    "        self.cv = cv\n",
    "    def Plot_cross_validation_results(self):\n",
    "        pca = PCA()\n",
    "\n",
    "        X_reduced = pca.fit_transform(self.pre_process.fit_transform(self.X_train))\n",
    "                                \n",
    "\n",
    "        #define cross validation method\n",
    "        #cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "        regr = LinearRegression()\n",
    "        mse = []\n",
    "\n",
    "        # Calculate MSE with only the intercept\n",
    "        score = -1*model_selection.cross_val_score(regr,\n",
    "           np.ones((len(X_reduced),1)), self.y_train, cv=self.cv,\n",
    "           scoring='neg_mean_squared_error').mean()    \n",
    "        mse.append(score)\n",
    "\n",
    "        # Calculate MSE using cross-validation, adding one component at a time\n",
    "        for i in np.arange(1, 6):\n",
    "            score = -1*model_selection.cross_val_score(regr,\n",
    "               X_reduced[:,:i], self.y_train, cv=self.cv, scoring='neg_mean_squared_error').mean()\n",
    "            mse.append(score)\n",
    "    \n",
    "        # Plot cross-validation results    \n",
    "        plt.plot(mse)\n",
    "        plt.xlabel('Number of Principal Components')\n",
    "        plt.ylabel('MSE')\n",
    "        plt.title('CDD')\n",
    "        variance_ratio = np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)\n",
    "        print(variance_ratio)\n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_predictor_variables = Scale_predictor_variables(X_train,y_train,pre_process,cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scale_predictor_variables.Plot_cross_validation_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scale_predictor_variables = Scale_predictor_variables(X_1train,y_1train,pre_process_2,cv_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scale_predictor_variables.Plot_cross_validation_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scoring:\n",
    "    def __init__(self,pre_process,X_train,y_train):\n",
    "        self.pre_process = pre_process\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "    def Score(self):\n",
    "        ## data \n",
    "        ########################################################################\n",
    "        model_1 = RandomForestRegressor(max_depth=15,random_state=0)\n",
    "        model_2 = LinearRegression(fit_intercept=True)\n",
    "        model_3 = Ridge(alpha=5)\n",
    "        model_4 = Lasso(alpha=10)\n",
    "        model_5 = SVR(C=2.5, epsilon=0.5)\n",
    "        model_6 = GradientBoostingRegressor(random_state=0)\n",
    "\n",
    "        MSE = []\n",
    "        R2 = []\n",
    "        for mymodels in [model_1,model_2,model_3,model_4,model_5,model_6]:\n",
    "            model_pipeline = Pipeline(steps=[('pre_processing',self.pre_process),\n",
    "                                 ('model', mymodels)\n",
    "                                 ])\n",
    "            model_pipeline.fit(self.X_train,self.y_train)\n",
    "            MSE.append(mean_squared_error(self.y_train,model_pipeline.predict(self.X_train))**0.5)\n",
    "            R2.append(r2_score(self.y_train,model_pipeline.predict(self.X_train)))\n",
    "    \n",
    "        print(np.round(MSE,2))   \n",
    "        print(np.round(R2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scoring1 = Scoring(pre_process,X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Scoring1.Score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scoring2 = Scoring(pre_process_2,X_1train,y_1train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scoring2.Score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model_select:\n",
    "    def __init__(self,X_train,y_train,X_test, y_test):\n",
    "        self.X_train = X_train\n",
    "        self.y_train = y_train\n",
    "        self.X_test = X_test\n",
    "        self.y_test = y_test\n",
    "    def model_selection(self):\n",
    "        models = []\n",
    "        models_1 = [\"Ridge\",\"Lasso\",\"LinearRegression\"]\n",
    "        models_2 = [\"RandomForestRegressor\",\"GradientBoostingRegressor\"]\n",
    "        model_3 = [\"SVR\"]\n",
    "        models += models_1 + models_2 + model_3\n",
    "        models_dictionary = {\"Ridge\":Ridge(),\"Lasso\":Lasso(),\"LinearRegression\":LinearRegression(fit_intercept=True),\n",
    "                             \"RandomForestRegressor\":RandomForestRegressor(random_state=0),\"GradientBoostingRegressor\":GradientBoostingRegressor(random_state=0),\n",
    "                            \"SVR\":SVR(epsilon=0.5)}\n",
    "        models_score = {}\n",
    "        \n",
    "        # Tuning of parameters for regression by cross-validation\n",
    "        K = 5               # Number of cross valiations\n",
    "        \n",
    "        for model in models:\n",
    "            if model in models_1:\n",
    "                \n",
    "                pipe = Pipeline([\n",
    "                ('scaler', StandardScaler()),\n",
    "                ('reduce_dim', PCA()),\n",
    "                ('regressor', models_dictionary[model])\n",
    "                ])\n",
    "                pipe = pipe.fit(self.X_train, self.y_train)\n",
    "                n_features_to_test = np.arange(1, 13)\n",
    "                alpha_to_test = 2.0**np.arange(-6, +6)\n",
    "            \n",
    "                if model == \"LinearRegression\":\n",
    "                    params = {'reduce_dim__n_components': n_features_to_test,\n",
    "                    'scaler' : [StandardScaler(), RobustScaler()]}\n",
    "                else:\n",
    "                    params = {'reduce_dim__n_components': n_features_to_test,\n",
    "                    'regressor__alpha': alpha_to_test,\n",
    "                    'scaler' : [StandardScaler(), RobustScaler()]}\n",
    "                gridsearch = GridSearchCV(pipe, params, verbose=1,cv = K).fit(self.X_train, self.y_train)\n",
    "                \n",
    "            elif model in models_2:\n",
    "                \n",
    "                if model == \"RandomForestRegressor\":\n",
    "                    \n",
    "                    model_estimator =  models_dictionary[model]\n",
    "                    params={'n_estimators':[20,30,40,60,100], 'max_depth': \n",
    "                    [5,10,15,20],'max_features':[2,5,8]}\n",
    "                    \n",
    "                     \n",
    "                else:\n",
    "                    model_estimator =  models_dictionary[model]\n",
    "                    \n",
    "                    params = {'learning_rate': [0.01,0.02,0.03,0.04],\n",
    "                    'subsample'    : [0.9, 0.5, 0.2, 0.1],\n",
    "                    'n_estimators' : [100,500,1000, 1500],\n",
    "                    'max_depth'    : [4,6,8,10]\n",
    "                     }\n",
    "                \n",
    "                gridsearch = GridSearchCV(estimator = model_estimator,param_grid = params, cv = K, n_jobs=-1).fit(self.X_train, self.y_train)\n",
    "            else:\n",
    "                parameters = {'gamma': [1e-4, 1e-3, 0.01, 0.1, 0.2, 0.5, 0.6, 0.9],'C': [1, 2.5, 5, 10, 100, 1000, 10000]}\n",
    "                gridsearch = GridSearchCV(models_dictionary[model], parameters, cv = K).fit(self.X_train, self.y_train)\n",
    "             \n",
    "            print(\" Results from Grid Search:\",model)\n",
    "            print(\"\\n The best estimator across ALL searched params:\\n\",gridsearch.best_estimator_)\n",
    "            print(\"\\n The best score across ALL searched params:\\n\",gridsearch.best_score_)\n",
    "            print(\"\\n The best parameters across ALL searched params:\\n\",gridsearch.best_params_)\n",
    "            print('\\n Final score is: ', gridsearch.score(self.X_test, self.y_test))\n",
    "            print(\"\")\n",
    "            models_score[model] = gridsearch.score(self.X_test, self.y_test)\n",
    "        self.models_score = models_score\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_select = Model_select(X_train,y_train,X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_select.model_selection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_select.models_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
