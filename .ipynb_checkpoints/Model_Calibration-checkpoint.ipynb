{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Calibration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Import packages and set working directory</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'plotly'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c356c114865f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGradientBoostingRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpress\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplotly\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure_factory\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mff\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'plotly'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from time import time\n",
    "#from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline,FeatureUnion\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import seaborn as sb\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wdir = \"/Users/emmanuel_mj/Documents/GitHub/mda_heatwaves-cameroon/DataPrep/Data\"\n",
    "os.chdir(wdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Get data and Explore</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean = pd.read_csv('MainData_Scaled.csv', sep=\"|\", header=0)\n",
    "data_clean1 = pd.read_csv('MainData_NotScaled.csv', sep=\"|\", header=0)\n",
    "data_clean2 = pd.read_csv('MainData_Scaled_OutliersRemoved.csv', sep=\"|\", header=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main = data_clean.loc[:,\"CDD\":\"CITY_REL_WATER\"]\n",
    "df_main1 = data_clean1.loc[:,\"CDD\":\"CITY_REL_WATER\"]\n",
    "df_main2 = data_clean2.loc[:,\"CDD\":\"CITY_REL_WATER\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for the null values\n",
    "df_main.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main1.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_2 = df_main.drop(['URB_AREA_HINTER', 'GDP_PC_REAL_PPP','POP_TOT_GI'], axis=1)\n",
    "#df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main = df_main.where(pd.notna(df_main), df_main.mean(), axis=\"columns\")\n",
    "df_main1 = df_main1.where(pd.notna(df_main1), df_main1.mean(), axis=\"columns\")\n",
    "df_main2 = df_main2.where(pd.notna(df_main2), df_main2.mean(), axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main1.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_main1.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(data_clean1, x=\"CDD\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(data_clean2, x=\"CDD\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pre_process = ColumnTransformer(remainder='passthrough',\n",
    "                                transformers=[('drop_columns', 'drop', ['GDP_PC_REAL_PPP',\n",
    "                                                                        'URB_AREA_HINTER',\n",
    "                                                                        'POP_TOT_GI'\n",
    "                                                                       ]),\n",
    "                                              ('impute_FRAGMENTATION', SimpleImputer(strategy='mean'), ['FRAGMENTATION']),\n",
    "                                              ('impute_T_Y0_14_SH_NAT', SimpleImputer(strategy='mean'), ['T_Y0_14_SH_NAT']),\n",
    "                                              ('impute_T_Y15_64_SH_NAT', SimpleImputer(strategy='mean'), ['T_Y15_64_SH_NAT']),\n",
    "                                              ('impute_T_Y65_MAX_SH_NAT', SimpleImputer(strategy='mean'), ['T_Y65_MAX_SH_NAT']),\n",
    "                                              ('impute_PWM_EX_CORE', SimpleImputer(strategy='mean'), ['PWM_EX_CORE'])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable importance/selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new arrays for variable importance scaled data with outliers\n",
    "y_1 = df_main.loc[:,\"CDD\"]\n",
    "X_1 = df_main.loc[:,\"URB_AREA\":\"CITY_REL_WATER\"]\n",
    "X_1train, X_1test, y_1train, y_1test = train_test_split(X_1, y_1, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lasso = LassoCV().fit(pre_process.fit_transform(X_1train), y_train)\n",
    "#lasso_pipeline = Pipeline(steps=[('pre_processing',pre_process),\n",
    "#                                ('lasso', LassoCV(cv=5, random_state=0))\n",
    "#                                 ])\n",
    "#lasso_pipeline.fit(X_1train,y_2train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(X_1train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lasso = LassoCV(cv=10, random_state=0,max_iter=10000).fit(X_1train, y_1train)\n",
    "importance = np.abs(lasso.coef_)\n",
    "feature_names = np.array(X_1train.columns)\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.bar(height=importance, x=feature_names)\n",
    "plt.title(\"Feature importances via coefficients\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new arrays for variable importance scaled data with outliers removed\n",
    "y_2 = df_main2.loc[:,\"CDD\"]\n",
    "X_2 = df_main2.loc[:,\"URB_AREA\":\"CITY_REL_WATER\"]\n",
    "X_2train, X_2test, y_2train, y_2test = train_test_split(X_2, y_2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_2 = LassoCV(cv=10, random_state=0,max_iter=10000).fit(X_2train, y_2train)\n",
    "importance_2 = np.abs(lasso_2.coef_)\n",
    "feature_names = np.array(X_2train.columns)\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.bar(height=importance_2, x=feature_names)\n",
    "plt.title(\"Feature importances via coefficients\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array for calibration with outliers\n",
    "y = df_main.loc[:,\"CDD\"]\n",
    "X = df_main.loc[:,\"URB_AREA\":\"CITY_REL_WATER\"]\n",
    "rng = np.random.RandomState(0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=rng)\n",
    "\n",
    "# Array for calibration with outliers removed\n",
    "y_1 = df_main2.loc[:,\"CDD\"]\n",
    "X_1 = df_main2.loc[:,\"URB_AREA\":\"CITY_REL_WATER\"]\n",
    "rng = np.random.RandomState(0)\n",
    "X_1train, X_1test, y_1train, y_1test = train_test_split(X_1, y_1, random_state=rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_process = ColumnTransformer(remainder='passthrough',\n",
    "                                transformers=[('drop_columns', 'drop', ['T_Y0_14_SH_NAT',\n",
    "                                                                        'URB_AREA',\n",
    "                                                                        'T_Y65_MAX_SH_NAT',\n",
    "                                                                        'POP_DEN'\n",
    "                                                                       ])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_process_2 = ColumnTransformer(remainder='passthrough',\n",
    "                                transformers=[('drop_columns', 'drop', ['T_Y0_14_SH_NAT',\n",
    "                                                                        'URB_AREA',\n",
    "                                                                        'T_Y15_64_SH_NAT',\n",
    "                                                                        'CITY_REL_ROADS',\n",
    "                                                                        'TREECOVER_SHARE_CORE'\n",
    "                                                                       ])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale predictor variables\n",
    "pca = PCA()\n",
    "\n",
    "X_reduced = pca.fit_transform(pre_process.fit_transform(X_train))\n",
    "                                \n",
    "\n",
    "#define cross validation method\n",
    "cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "regr = LinearRegression()\n",
    "mse = []\n",
    "\n",
    "# Calculate MSE with only the intercept\n",
    "score = -1*model_selection.cross_val_score(regr,\n",
    "           np.ones((len(X_reduced),1)), y_train, cv=cv,\n",
    "           scoring='neg_mean_squared_error').mean()    \n",
    "mse.append(score)\n",
    "\n",
    "# Calculate MSE using cross-validation, adding one component at a time\n",
    "for i in np.arange(1, 6):\n",
    "    score = -1*model_selection.cross_val_score(regr,\n",
    "               X_reduced[:,:i], y_train, cv=cv, scoring='neg_mean_squared_error').mean()\n",
    "    mse.append(score)\n",
    "    \n",
    "# Plot cross-validation results    \n",
    "plt.plot(mse)\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('CDD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale predictor variables\n",
    "pca_2 = PCA()\n",
    "\n",
    "X_1reduced = pca_2.fit_transform(pre_process_2.fit_transform(X_1train))\n",
    "                                \n",
    "\n",
    "#define cross validation method\n",
    "cv_1 = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "\n",
    "regr_1 = LinearRegression()\n",
    "mse_1 = []\n",
    "\n",
    "# Calculate MSE with only the intercept\n",
    "score_1 = -1*model_selection.cross_val_score(regr_1,\n",
    "           np.ones((len(X_1reduced),1)), y_1train, cv=cv,\n",
    "           scoring='neg_mean_squared_error').mean()    \n",
    "mse_1.append(score_1)\n",
    "\n",
    "# Calculate MSE using cross-validation, adding one component at a time\n",
    "for i in np.arange(1, 6):\n",
    "    score_1 = -1*model_selection.cross_val_score(regr_1,\n",
    "               X_1reduced[:,:i], y_1train, cv=cv_1, scoring='neg_mean_squared_error').mean()\n",
    "    mse_1.append(score)\n",
    "    \n",
    "# Plot cross-validation results    \n",
    "plt.plot(mse_1)\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('MSE_1')\n",
    "plt.title('CDD')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cumsum(np.round(pca_2.explained_variance_ratio_, decimals=4)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data with outliers\n",
    "########################################################################\n",
    "model_1 = RandomForestRegressor(max_depth=15,random_state=0)\n",
    "model_2 = LinearRegression(fit_intercept=True)\n",
    "model_3 = Ridge(alpha=5)\n",
    "model_4 = Lasso(alpha=10)\n",
    "model_5 = SVR(C=2.5, epsilon=0.5)\n",
    "model_6 = GradientBoostingRegressor(random_state=0)\n",
    "\n",
    "MSE = []\n",
    "R2 = []\n",
    "for mymodels in [model_1,model_2,model_3,model_4,model_5,model_6]:\n",
    "    model_pipeline = Pipeline(steps=[('pre_processing',pre_process),\n",
    "                                 ('model', mymodels)\n",
    "                                 ])\n",
    "    model_pipeline.fit(X_train,y_train)\n",
    "    MSE.append(mean_squared_error(y_train,model_pipeline.predict(X_train))**0.5)\n",
    "    R2.append(r2_score(y_train,model_pipeline.predict(X_train)))\n",
    "    \n",
    "print(np.round(MSE,2))   \n",
    "print(np.round(R2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#models = model_pipeline.steps[1][0]\n",
    "model_pipeline.steps[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## data with outliers removed\n",
    "############################################################################\n",
    "model_1_1 = RandomForestRegressor(max_depth=15,random_state=0)\n",
    "model_2_1 = LinearRegression(fit_intercept=True)\n",
    "model_3_1 = Ridge(alpha=5)\n",
    "model_4_1 = Lasso(alpha=10)\n",
    "model_5_1 = SVR(C=2.5, epsilon=0.5)\n",
    "model_6_1 = GradientBoostingRegressor(random_state=0)\n",
    "\n",
    "MSE_1 = []\n",
    "R2_1 = []\n",
    "for mymodels_1 in [model_1_1,model_2_1,model_3_1,model_4_1,model_5_1,model_6_1]:\n",
    "    model_pipeline_1 = Pipeline(steps=[('pre_processing',pre_process_2),\n",
    "                                 ('model_1', mymodels_1)\n",
    "                                 ])\n",
    "    model_pipeline_1.fit(X_1train,y_1train)\n",
    "    MSE_1.append(mean_squared_error(y_1train,model_pipeline_1.predict(X_1train))**0.5)\n",
    "    R2_1.append(r2_score(y_1train,model_pipeline_1.predict(X_1train)))\n",
    "    \n",
    "print(np.round(MSE_1,2))   \n",
    "print(np.round(R2_1,2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
