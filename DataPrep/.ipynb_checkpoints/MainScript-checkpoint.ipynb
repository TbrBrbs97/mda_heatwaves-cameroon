{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Prediction of cooling degree days for metropolitan areas: prioritising corrective measures to deal with heat waves* (Cameroon Group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> This notebook is attached to a report with the same title, as part of the course *Modern Data Analytics [G0Z39a]*. The goal of this report is to, given various datasets, gain a rapid insight into how different variables (territorial, socio-economic or environmental) can be used to predict a metropolitan area's cooling degree days (CDD). This parameter, coined by the OECD, refers to the amount of days per year in which threshold room temperature is exceeded, and to what extent. The indicator thus expresses an energy demand for the cooling of buildings. As data scientists, we'll be looking to what parameters are most contributing to that phenomena, and what regression models can be used to approximate the observed variations in CDD best. This notebook pertains the following structure: \n",
    "\n",
    "1. Import packages\n",
    "2. Data preparation\n",
    "    * Metropolitan data\n",
    "    * National data\n",
    "    * Data formatation\n",
    "    * Data Cleaning\n",
    "    * Outlier detection and removal\n",
    "3. Data Analysis\n",
    "   * Data Exploration\n",
    "   * Clustering Analysis\n",
    "   * Time Series Analysis\n",
    "4. Model Calibration\n",
    "   * Variable Selection\n",
    "   * Modeling pipeline\n",
    "5. Model Validation\n",
    "    \n",
    "    \n",
    "</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Packages\n",
    "import numpy as np\n",
    "import sys\n",
    "import pandas as pd\n",
    "import wbgapi as wb\n",
    "import sklearn.preprocessing\n",
    "import seaborn as sns\n",
    "from pandas import DataFrame\n",
    "from scipy.stats import shapiro\n",
    "from sklearn.covariance import MinCovDet\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.tsa.api as smt\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as scs\n",
    "import scipy as scy\n",
    "import pandas as pd\n",
    "from time import time\n",
    "#from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline,FeatureUnion\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso, PoissonRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import seaborn as sb\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.cluster import KMeans\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "#Own .py-scripts\n",
    "%run Country_City_Correspondance.py\n",
    "%run Modeling_Functions.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Preparation\n",
    "## 2.1 Import Data\n",
    "<b> Note that a special webservice 'wbgapi' is used to import data from the WorldBank, to assist the formatting process </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import OECD-data from CSV-files\n",
    "\n",
    "#Metropolitan-based data\n",
    "oecd_data1 = pd.read_csv('Data/CITIES_POPULATION.csv', sep=\"|\", header=0)\n",
    "oecd_data2 = pd.read_csv('Data/CITIES_AGE.csv', sep=\"|\", header=0)\n",
    "oecd_data3 = pd.read_csv('Data/CITIES_ECONOMY.csv', sep=\"|\", header=0)\n",
    "oecd_data4 = pd.read_csv('Data/CITIES_LABOUR.csv', sep=\"|\", header=0)\n",
    "oecd_data5 = pd.read_csv('Data/CITIES_TERRITORY.csv', sep=\"|\", header=0)\n",
    "oecd_data6 = pd.read_csv('Data/CITIES_ENVIRONMENT.csv', sep=\"|\", header=0)\n",
    "\n",
    "#Country-based data: this will later be translated into metropolitan data\n",
    "country_codes = pd.read_excel('Data\\COUNTRY_CODE.xls')\n",
    "country_inlandwater = pd.read_csv('Data\\COUNTRY_INLAND_WATER.csv')\n",
    "country_evapotranspiration = pd.read_csv('Data\\COUNTRY_EVAPOTRANSPIRATION.csv')\n",
    "country_roaddensity = pd.read_csv('Data\\COUNTRY_ROAD_DENSITY.csv')\n",
    "country_area = wb.data.DataFrame('AG.LND.TOTL.K2',time=2018,labels=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Metropolitan data (OECD) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put all OECD-data into dataframe and append all dataframes\n",
    "oecd_data_df = [oecd_data1, oecd_data2, oecd_data3, oecd_data4, oecd_data5, oecd_data6]\n",
    "# Call concat method\n",
    "oecd_df = pd.concat(oecd_data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>The data is not structured in the way we want to process it (too many columns, multiple rows per sample,...). Above all, the data needs to be structured in a comprehensible format (rows=samples, columns=variables). Furthermore, a linear combination of some variables will be useful for later analysiss and merging with country-based data.\n",
    "\n",
    "Priority is given to the primary process, such that the final structuring can be done on the bulk of the adapted data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oecd_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 Variable creation as a linear combination of existing variabels\n",
    "\n",
    "<b> A variable 'Construction ratio' needs to be created, in order to fetch the relative built environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## CONSTRUCTION RATIO\n",
    "\n",
    "#pick the Territory dataset, calculate the construction densiy ratio\n",
    "oecd_data5_1 = oecd_data5.loc[oecd_data5['Variables']=='Urbanised area (built-up area or land for urban use in km2)']\n",
    "oecd_data5_2 = oecd_data5.loc[oecd_data5['Variables']=='Metropolitan area total land area']\n",
    "\n",
    "#oecd_data5_1 has 1336 rows, while oecd_data5_2 has 668 rows (exactly two time of the first dataset), \n",
    "#we could try to combine the two datasets by the common column \"METRO_ID\",\n",
    "\n",
    "oecd_data5_improved = pd.merge(oecd_data5_1,oecd_data5_2,how='inner',on='METRO_ID')\n",
    "\n",
    "#and then calculate the constuction density ratio = \"Urbanised area\" / \"Metropolitan area total land area\"\n",
    "oecd_data5_improved['construction_ratio'] = oecd_data5_improved['Value_x'] / oecd_data5_improved['Value_y']\n",
    "oecd_data5_improved.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.2 Rename columnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary where key = old name and value = new name\n",
    "dict = {'METRO_ID': 'metroId',\n",
    "        'Metropolitan areas': 'metropolitanAreas',\n",
    "        'VAR': 'var',\n",
    "       'Variables' : 'variables',\n",
    "        'TIME' : 'time',\n",
    "        'Year' : 'year',\n",
    "        'Unit Code': 'unitCode',\n",
    "        'Unit' : 'unit',\n",
    "        'PowerCode Code' : 'powerCodeCode',\n",
    "        'PowerCode': 'powerCode',\n",
    "        'Reference Period Code' : 'referencePeriodCode',\n",
    "        'Reference Period' : 'referencePeriod',\n",
    "        'Value':'value',\n",
    "        'Flag Codes' : 'flagCodes',\n",
    "        'Flags': 'flags'\n",
    "       }\n",
    "  \n",
    "# call rename () method\n",
    "oecd_df.rename(columns=dict,\n",
    "          inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the data\n",
    "print(oecd_df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.3 Add longitude and latitude to join data\n",
    "\n",
    "<b> In order to add the possibility of visualisation, longitude and lattitude is computed for every metropolitan area\n",
    "\n",
    "Strings that represent metropolitan areas are often different, which makes it difficult to join data from different sources. For example, \"Lafayette (IN)\" and \"Lafayette, IN\" and \"Lafayette\". Therefore, longitude and latitude are added as a unique identifier to join data. \n",
    "\n",
    "This is performed by *LatLongGeneration.py*. Since the script carries out computationally intensive tasks, it is left out of this code and thus only the result is imported as a .csv-file. </b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Now, we add longitudes and latitudes to metropolitan areas </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_metropolitan = pd.read_csv('Data/CITIES_COORD.csv', header=0, sep=\"|\", doublequote=True)\n",
    "df_metropolitan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Merge and create temporary dataset, without country-based statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the dataframes\n",
    "df_oecdCoord = pd.merge(oecd_df, df_metropolitan, on=\"metropolitanAreas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Country-based statistics (Worldbank, OECD)\n",
    "\n",
    "<b> A first step in the process of adding national statistics as proxys for metropolitan statistics, is to built a metropolitan area-country correspondance matrix. The idea being that all national indicators can then be added to that dataframe, only then to be weighed correctly according to the relative surface of a metropolitan area in a country.\n",
    "\n",
    "This of course assumes all variables (such as inland water, see later) to be homogeneously present over an entire country. Therefore this disaggregation process entails overestimation in some cases, underestimation in others. We consider it elligeble proxy's nonetheless.</b>\n",
    "\n",
    "### 2.3.1 Metropolitan area - country correspondance and exception handling\n",
    "\n",
    "<b> This process is partly automated by making use of the abbreviations utilized by OECD. Exception handling is however necessary, since some abbreviations for countries differ per dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OECD set of population contains the best reference in terms of number of unique cities\n",
    "cities_set = set(oecd_data1['METRO_ID'])\n",
    "\n",
    "country_dict = country_codes[['CODE','Country']].to_dict('records')\n",
    "country_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run external .py-script which attaches country names to each metropolitan area\n",
    "df_country_city = country_cities_correspondance(country_dict,cities_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3.2 Disaggregating national statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare extra variables for merging with data\n",
    "\n",
    "#Inland water\n",
    "inland_water_15 = country_inlandwater[country_inlandwater['Year']==2015]\n",
    "inland_water_15 = inland_water_15[inland_water_15['MEAS']=='PCNT']\n",
    "inland_water_15 = inland_water_15[['Country','Value']]\n",
    "\n",
    "#Road density\n",
    "road_density = country_roaddensity[['Country','Amount']]\n",
    "\n",
    "#Evapotranspiration\n",
    "evapotrans = country_evapotranspiration[['Country','Value']]\n",
    "\n",
    "#merge first two\n",
    "road_and_water = pd.merge(inland_water_15,road_density,how=\"inner\",on='Country')\n",
    "road_water_evapo = pd.merge(road_and_water,evapotrans,how=\"inner\",on='Country')\n",
    "\n",
    "#merge with main dataframe\n",
    "df_country_city_ext = df_country_city.reset_index().merge(road_water_evapo, how=\"left\",on='Country').set_index('index')\n",
    "df_country_city_ext.rename(columns = {'Value_x':'CNTRY_INLAND_WATER','Amount':'CNTRY_ROAD_DENS','Value_y':'EVAPOTRANS'}, inplace = True)\n",
    "df_country_city_ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we add relative territory of the city\n",
    "\n",
    "cities_surface = oecd_data5[oecd_data5['VAR']=='SURF']\n",
    "cities_surface = cities_surface[['METRO_ID','Value']]\n",
    "cities_surface.rename(columns = {'METRO_ID':'index'}, inplace = True)\n",
    "cities_surface\n",
    "\n",
    "country_area = country_area[['Country','AG.LND.TOTL.K2']]\n",
    "\n",
    "#merge both with the main dataframe\n",
    "merged_data = df_country_city_ext.reset_index().merge(country_area, how=\"left\",on='Country').set_index('index')\n",
    "merged_data = merged_data.reset_index().merge(cities_surface, how=\"left\",on='index').set_index('index')\n",
    "\n",
    "merged_data.rename(columns = {'AG.LND.TOTL.K2':'CNTRY_TOT_AREA','Value':'CITY_TOT_SURF'}, inplace = True)\n",
    "merged_data\n",
    "\n",
    "#Calculate relative city surface\n",
    "merged_data['CITY_REL_SURF'] = (merged_data['CITY_TOT_SURF'] / merged_data['CNTRY_TOT_AREA'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use relative city surface to weigh and disaggregate national statistics\n",
    "merged_data['CITY_REL_ROADS'] = (merged_data['CNTRY_ROAD_DENS'].astype(float) * merged_data['CITY_REL_SURF'])\n",
    "merged_data['CITY_REL_WATER'] = (merged_data['CNTRY_INLAND_WATER'] * merged_data['CITY_REL_SURF'])\n",
    "merged_data['CITY_EVAPOTRANS'] = (merged_data['EVAPOTRANS'] * merged_data['CITY_REL_SURF'])\n",
    "\n",
    "#Prepare for merge with city-based statistics\n",
    "merged_data.rename(index = {'index':'metroId'}, inplace = True)\n",
    "merged_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4. Transpose dataset, merge datasets\n",
    "\n",
    "<b> Now, we're going to transform the matrix such that on the rows we find countries, whereas on the columns we find variables.\n",
    "In principle, such matrix could be built for every year. A first step is to delete all metadata related variables. Next, only the useful variables are kept. A final step is to eliminate the time dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first get rid of all metadata-related columns\n",
    "df_oecd_col_red = df_oecdCoord[['metroId','metropolitanAreas','latitude','longitude','var','variables','year','value']]\n",
    "\n",
    "#first test some things on 1 year\n",
    "df_oecd_col_red_14 = df_oecd_col_red[df_oecd_col_red['year']==2014]\n",
    "#df_oecd_col_red_14.head()\n",
    "\n",
    "#group by metropolitanAreas\n",
    "df_total_14 = df_oecd_col_red_14[['metroId','metropolitanAreas','latitude','longitude','var','value']]\n",
    "\n",
    "df_total_14 = df_total_14.pivot_table(index=['metroId','metropolitanAreas','latitude','longitude'], \n",
    "                    columns=['var',], \n",
    "                    values='value', \n",
    "                    aggfunc='mean')\n",
    "df_total_14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Once metadata is cleared, and the dataset is pivotted, it is time to perform a preliminary QUALITATIVE variable selection, since the OECD-data contains many variables already that are highly unlikely to contribute to the model given the scope of this research (e.g. vulnerability to pollution (PWM_EX_CORE) is taken in stead of specific vulnerability to specific pollution). Preference is furthermore given to aggregate and relative variables (e.g. population density, or share of elderly in stead of absolute numbers). </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we only select the useful variables: i.e. aggregate variables, relative variables, ...\n",
    "df_selection_14 = df_total_14[['CDD','URB_AREA','URB_AREA_CORE',\n",
    "                               'URB_AREA_HINTER','FRAGMENTATION',\n",
    "                               'GDP_PC_REAL_PPP','PARTIC_RA_15_64','POP_DEN',\n",
    "                              'POP_TOT_GI','PWM_EX_CORE','T_Y0_14_SH_NAT',\n",
    "                               'T_Y15_64_SH_NAT','T_Y65_MAX_SH_NAT'\n",
    "                              ]]\n",
    "df_selection_14\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> We now add some variables seperately, notably: the construction data from 1.2.2, and the tree share cover from the territory dataset (OECD). It is noted that the 'Tree share cover' data is from 2015, whereas the general dataset contains data from 2014. Although tree cover data is available from 2004, such that an interpolation would be possible, we opted not to do so, since interpolation or extrapolation entails callibrating a growth model (linear, logarithmic, exponential?). Since such computations would induce more errors.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Next we add (1) construction ratio and (2) Tree cover manually as variables\n",
    "\n",
    "#Construction_data\n",
    "construction_ratio_14 = oecd_data5_improved[oecd_data5_improved['Year_x']==2014]\n",
    "construction_ratio_14 = construction_ratio_14[['METRO_ID','construction_ratio']]\n",
    "\n",
    "#Tree_cover\n",
    "tree_cover = oecd_data6[oecd_data6['VAR']=='TREECOVER_SHARE_CORE']\n",
    "tree_cover = tree_cover[tree_cover['Year']==2015]\n",
    "tree_cover = tree_cover[['METRO_ID','Value']]\n",
    "\n",
    "tree_and_construction = pd.merge(construction_ratio_14,tree_cover,how='inner',on='METRO_ID')\n",
    "tree_and_construction = tree_and_construction.rename(columns = {'METRO_ID':'metroId','construction_ratio': 'CONSTR_RAT', 'Value': 'TREECOVER_SHARE_CORE'}, inplace = False)\n",
    "\n",
    "#Merge with main dataset\n",
    "\n",
    "df_extended_14 = df_selection_14.reset_index().merge(tree_and_construction, how='left',on='metroId')\n",
    "df_extended_14\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, the country-based statistics are to be added to the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a selection of only the variables to be merged\n",
    "national_data = merged_data[['CITY_REL_ROADS','CITY_REL_WATER','CITY_EVAPOTRANS']]\n",
    "national_data.reset_index(level=0, inplace=True)\n",
    "national_data = national_data.rename(columns={'index':'metroId'})\n",
    "national_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data = df_extended_14.reset_index().merge(national_data, how='left',on='metroId')\n",
    "complete_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.5. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "complete_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Samples for which we don't have any data on the Cooling Degree Days, are to be dropped anyway since they will not be of any use as observations for the the dependent variable. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "complete_data.dropna(subset=['CDD'],inplace=True)\n",
    "# drop values that have missing data for cooling days\n",
    "complete_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# We further reduce the dataset extent by only allowing columns to exhibit at least 80% real values.\n",
    "complete_data.dropna(thresh=0.8*len(complete_data),axis=1,inplace=True)\n",
    "\n",
    "#replace missing values with mean of the specific column\n",
    "complete_data_1 = complete_data.iloc[:,5:].where(pd.notna(complete_data), complete_data.mean(), axis=\"columns\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data_1.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data_1.insert(0, 'metroId', complete_data['metroId'])\n",
    "complete_data_1.insert(1,'metropolitanAreas',complete_data['metropolitanAreas'])\n",
    "complete_data_1.insert(2,'lat',complete_data['latitude'])\n",
    "complete_data_1.insert(3,'long',complete_data['longitude'])\n",
    "\n",
    "data_clean1 = complete_data_1\n",
    "complete_data_1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.6 Outlier detection & removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "complete_data_1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapiro(complete_data_1['CDD'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> As the shapiro test yields a p-value smaller than 0.05, the null hypothesis that the variable CDD is normally distributed is rejected. Therefore, when detecting outliers, we should note that any method that asumes normal distribution (such as using  Z-scores) is to be disregarded.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(complete_data_1['CDD'])\n",
    "#axes-level function for histograms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> The Minimum Covariance Determinant covariance estimator is to be applied on Gaussian-distributed data,\n",
    "but could still be relevant on data drawn from a unimodal, symmetric distribution. \n",
    "It is not meant to be used with multi-modal data (the algorithm used to fit a MinCovDet object is likely to fail in such a case). One should consider projection pursuit methods to deal with multi-modal datasets. </b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(complete_data_1['CDD'])\n",
    "#axes-level function for histograms\n",
    "#unimodal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_test_mincov = MinCovDet(random_state=0).fit(complete_data_1.loc[:,'CDD':'CITY_REL_WATER']).mahalanobis(complete_data_1.loc[:,'CDD':'CITY_REL_WATER'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2\n",
    "crit_distance = chi2.ppf((1-0.01), df=complete_data_1.shape[1] - 1)\n",
    "#p-value that is less than .001 is considered to be an outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#idx = dist_test_mincov>crit_distance\n",
    "crit_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data_mahalonobis = complete_data_1.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_data_mahalonobis['mahalanobis'] = dist_test_mincov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "complete_data_mahalonobis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = dist_test_mincov>crit_distance\n",
    "np.sum(idx==True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Above represents the number of ouliers removed based on Mahalanobis Distance </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.histplot(complete_data_mahalonobis['CDD'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_clean2 = complete_data_mahalonobis[complete_data_mahalonobis['mahalanobis'] < crit_distance]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_clean1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_clean2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main1 = data_clean1.loc[:,\"CDD\":\"CITY_EVAPOTRANS\"]\n",
    "df_main2 = data_clean2.loc[:,\"CDD\":\"CITY_EVAPOTRANS\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_main1 = df_main1.where(pd.notna(df_main1), df_main1.mean(), axis=\"columns\")\n",
    "df_main2 = df_main2.where(pd.notna(df_main2), df_main2.mean(), axis=\"columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_main1.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(data_clean1, x=\"CDD\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(data_clean2, x=\"CDD\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Time series analysis: Checking for Autocorrelation in Cooling Days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oecd_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get CDD series for New York\n",
    "ny_data = oecd_df.loc[oecd_df[\"metropolitanAreas\"] == 'New York (Greater)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cooling_days_ny = ny_data.loc[ ny_data[\"var\"] == 'CDD',[\"year\", \"value\"]]\n",
    "cooling_days_ts = cooling_days_ny.set_index('year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsplot(y, lags=None, figsize=(10, 8), style='bmh'):\n",
    "    if not isinstance(y, pd.Series):\n",
    "        y = pd.Series(y)\n",
    "    with plt.style.context(style):    \n",
    "        fig = plt.figure(figsize=figsize)\n",
    "        layout = (3, 2)\n",
    "        ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2)\n",
    "        acf_ax = plt.subplot2grid(layout, (1, 0))\n",
    "        pacf_ax = plt.subplot2grid(layout, (1, 1))\n",
    "        qq_ax = plt.subplot2grid(layout, (2, 0))\n",
    "        pp_ax = plt.subplot2grid(layout, (2, 1))\n",
    "        \n",
    "        y.plot(ax=ts_ax)\n",
    "        ts_ax.set_title('Time Series Analysis Plots')\n",
    "        smt.graphics.plot_acf(y, lags=lags, ax=acf_ax, alpha=0.5)\n",
    "        smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax, alpha=0.5)\n",
    "        sm.qqplot(y, line='s', ax=qq_ax)\n",
    "        qq_ax.set_title('QQ Plot')        \n",
    "        scs.probplot(y, sparams=(y.mean(), y.std()), plot=pp_ax)\n",
    "\n",
    "        plt.tight_layout()\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "tsplot(np.diff(cooling_days_ts.value),lags=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> We can safely conclude that there is no autocorrelation in the Target variable CDD (Cooling days). However, we do note that the data is not suitable for time series analysis, due to the low resolution. </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Clustering\n",
    "<b> Cluster analysis is a statistical technique aimed to uncover groups (clusters) of observations that are homogeneous and separated from other groups. For this project, it might be useful to group a large number of cities by the values of their population and the number of yearly cooling days. The group of primary interest is the one that covers cities with the largest number of population and cooling days (the most vulnerable to heat waves). </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the rows with missing values\n",
    "dfc=df_main1.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choose the variables for the analysis\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_main1[['CONSTR_RAT','CDD']])\n",
    "\n",
    "df = scaler.transform(df_main1[['CONSTR_RAT','CDD']])\n",
    "X = df\n",
    "\n",
    "#df = df_main1[['CONSTR_RAT','CDD']]\n",
    "#df = df_main[['POP_DEN','CDD']]\n",
    "#df = df_main[['GDP_PC_REAL_PPP','CDD']]\n",
    "#df = df_main.loc[:,\"CDD\":\"CITY_EVAPOTRANS\"]\n",
    "#dfc = df_main[['POP_TOT_GI','CDD']]\n",
    "#df = df_main[['T_Y0_14_SH_NAT','CDD']]\n",
    "#df = df_main[['T_Y15_64_SH_NAT','CDD']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> K-means clustering method is applied. The desired number of clusters (k) needs to be specified in advance. The K-means algorithm then assigns each observation to exactly one of the k clusters. The algorithm is run multiple times from different random initial configurations aimed to find the best separated clusters. The K-means algorithm lacks flexibility in cluster shape and probabilistic cluster assignment. </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> For clustering, the data needs to be standardized given that the variables in the data set have different scales and variances. The dataset has already been standardized by using StandardScaler(). Clusters with centeroids are visualized then on the scatterplot. </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Kmeans for 2 clusters and fit it to the data\n",
    "cluster_with_scaling = KMeans(n_clusters=2, n_init=20,init='random',random_state=0)\n",
    "cluster_with_scaling.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=cluster_with_scaling.predict(X)\n",
    "fig,ax = plt.subplots(figsize=(10,10))\n",
    "idx_1 = y==1\n",
    "idx_0 = y==0\n",
    "color_1='blue'\n",
    "color_0 ='red'\n",
    "ax.scatter(X[idx_1,0],X[idx_1,1],s=40,color=color_1,label=None)\n",
    "ax.scatter(X[idx_0,0],X[idx_0,1],s=40,color=color_0,label=None)\n",
    "\n",
    "ax.scatter(cluster_with_scaling.cluster_centers_[:,0],cluster_with_scaling.cluster_centers_[:,1],s=120,\n",
    "           color='k',marker='s',\n",
    "           label='Centroids of the Clusters')\n",
    "\n",
    "ax.set_xlabel('Construction Rate')\n",
    "ax.set_ylabel('Cooling Days');\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> As seen from the scatter plots, there are cities that could be susceptible to heat waves and damages associated with heatwaves more than cities in other groups. For example, cities with population larger than 135 thousands and cooling days largerthan 730 days (calculated by using the values of the mean and standard deviation of the dataset before scalling applied) </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model Calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1. Variable selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 4.1. Variable selection# create new arrays for variable importance scaled data with outliers\n",
    "y_1 = df_main1.loc[:,\"CDD\"]\n",
    "X_1 = df_main1.loc[:,\"URB_AREA\":\"CITY_EVAPOTRANS\"]\n",
    "X_1train, X_1test, y_1train, y_1test = train_test_split(X_1, y_1,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "np.array(X_1train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialise a new scaling object\n",
    "sc=StandardScaler() \n",
    "\n",
    "# Set up the scaler just on the training set\n",
    "X_train = sc.fit_transform(X_1train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lasso = LassoCV(cv=10, random_state=0,max_iter=10000).fit(X_train, y_1train)\n",
    "importance = np.abs(lasso.coef_)\n",
    "feature_names = np.array(X_1train.columns)\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.bar(height=importance, x=feature_names)\n",
    "plt.title(\"Feature importances via coefficients\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create new arrays for variable importance scaled data with outliers removed\n",
    "y_2 = df_main2.loc[:,\"CDD\"]\n",
    "X_2 = df_main2.loc[:,\"URB_AREA\":\"CITY_EVAPOTRANS\"]\n",
    "X_2train, X_2test, y_2train, y_2test = train_test_split(X_2, y_2,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialise a new scaling object\n",
    "sc1=StandardScaler() \n",
    "\n",
    "# Set up the scaler just on the training set\n",
    "X_train = sc1.fit_transform(X_2train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasso_2 = LassoCV(cv=10, random_state=0,max_iter=10000).fit(X_train, y_2train)\n",
    "importance_2 = np.abs(lasso_2.coef_)\n",
    "feature_names = np.array(X_2train.columns)\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.bar(height=importance_2, x=feature_names)\n",
    "plt.title(\"Feature importances via coefficients\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array for calibration with outliers\n",
    "y = df_main1.loc[:,\"CDD\"]\n",
    "X = df_main1.loc[:,\"URB_AREA\":\"CITY_EVAPOTRANS\"]\n",
    "rng = np.random.RandomState(0)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.2, random_state=rng)\n",
    "\n",
    "# Array for calibration with outliers removed\n",
    "y_1 = df_main2.loc[:,\"CDD\"]\n",
    "X_1 = df_main2.loc[:,\"URB_AREA\":\"CITY_EVAPOTRANS\"]\n",
    "rng = np.random.RandomState(0)\n",
    "X_1train, X_1test, y_1train, y_1test = train_test_split(X_1, y_1,test_size=0.2,random_state=rng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. Modeling pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data with outliers\n",
    "pre_process = ColumnTransformer(remainder='passthrough',\n",
    "                                transformers=[('drop_columns', 'drop', ['T_Y0_14_SH_NAT',\n",
    "                                                                        'URB_AREA',\n",
    "                                                                        'T_Y15_64_SH_NAT',\n",
    "                                                                        'CONSTR_RAT',\n",
    "                                                                        'TREECOVER_SHARE_CORE',\n",
    "                                                                        'CITY_REL_WATER'\n",
    "                                                                       ])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data without outliers\n",
    "pre_process_2 = ColumnTransformer(remainder='passthrough',\n",
    "                                transformers=[('drop_columns', 'drop', ['T_Y0_14_SH_NAT',\n",
    "                                                                        'T_Y15_64_SH_NAT',\n",
    "                                                                        'URB_AREA'\n",
    "                                                                       ])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " cv_1 = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> both initiators below: *Scale_predictor_variables()* and *Scoring()* belong to the Modeling_Functions.py helper script </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_predictor_variables = Scale_predictor_variables(X_train,y_train,pre_process,cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "scale_predictor_variables.Plot_cross_validation_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scale_predictor_variables = Scale_predictor_variables(X_1train,y_1train,pre_process_2,cv_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "scale_predictor_variables.Plot_cross_validation_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scoring1 = Scoring(pre_process,X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Scoring1.Score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scoring2 = Scoring(pre_process_2,X_1train,y_1train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Scoring2.Score()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Model Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> The *Model_select()* is also to be found within Modeling_Functions.py </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_select = Model_select(X_train,y_train,X_test, y_test)\n",
    "#with outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_select.model_selection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_select.models_score\n",
    "#R-squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_select_1 = Model_select(X_1train,y_1train,X_1test, y_1test)\n",
    "#without outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_select_1.model_selection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_select_1.models_score\n",
    "#R-squared"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_model_1 = GradientBoostingRegressor(learning_rate=0.04, max_depth=4, random_state=0,\n",
    "                          subsample=0.5)\n",
    "selected_model_1.fit(X_1train,y_1train)\n",
    "pd.DataFrame({'Variable':X_1train.columns,\n",
    "              'Importance':selected_model_1.feature_importances_}).sort_values('Importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_model_2 = RandomForestRegressor(max_depth=20, max_features=2, n_estimators=80,\n",
    "                      random_state=0)\n",
    "selected_model_2.fit(X_1train,y_1train)\n",
    "pd.DataFrame({'Variable':X_1train.columns,\n",
    "              'Importance':selected_model_2.feature_importances_}).sort_values('Importance', ascending=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
